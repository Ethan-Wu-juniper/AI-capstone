{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Self-made Dataset (Emoji)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmmz3XO_vwY7"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "# import keras\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.layers import Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn import preprocessing\n",
        "from keras.utils.np_utils import to_categorical\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, SimpleRNN, Dense, Dropout, Activation, BatchNormalization, LayerNormalization\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import KFold\n",
        "import torch\n",
        "from sklearn.metrics import *\n",
        "import seaborn as sb\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwCmDyWN3xjR"
      },
      "source": [
        "### Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LO8fdAlG65cc"
      },
      "outputs": [],
      "source": [
        "DIM = [50,100,200,300]\n",
        "data = pd.read_csv('drive/MyDrive/emoji/new_dataset.csv',sep=',')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo3ar5KL30Pt"
      },
      "source": [
        "### Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69qvsTg83uuM"
      },
      "outputs": [],
      "source": [
        "def intialize_emb_matrix(file):\n",
        "  embedding_matrix = {}\n",
        "  for line in file:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    embedding = np.array(values[1:], dtype='float64')\n",
        "    embedding_matrix[word] = embedding\n",
        "\n",
        "  return embedding_matrix \n",
        "\n",
        "def get_emb_data(data, max_len, file, DIM):\n",
        "  embedding_matrix = intialize_emb_matrix(file)\n",
        "  embedding_data = np.zeros((len(data), max_len, DIM))\n",
        "  \n",
        "  for idx in range(data.shape[0]):\n",
        "    words_in_sentence = data[idx].split()\n",
        "    \n",
        "    for i in range(len(words_in_sentence)):\n",
        "      if embedding_matrix.get(words_in_sentence[i].lower()) is not None:\n",
        "        embedding_data[idx][i] = embedding_matrix[words_in_sentence[i].lower()]\n",
        "              \n",
        "  return embedding_data\n",
        "  \n",
        "def w2vector(data, DIM):\n",
        "  file = open('drive/MyDrive/emoji/glove.6B.'+str(DIM)+'d.txt',encoding='utf8')\n",
        "  X_train, y_train = data['Tweet'].values, data['Emoji'].values\n",
        "\n",
        "  le = preprocessing.LabelEncoder()\n",
        "  y_train = le.fit_transform(y_train)\n",
        "\n",
        "  MAX = 0\n",
        "  for i in range(X_train.shape[0]):\n",
        "    text = X_train[i].split()\n",
        "    MAX = max(MAX,len(text))\n",
        "    # print(X_train[i])\n",
        "  # print(MAX)\n",
        "\n",
        "  X_temb = get_emb_data(X_train, MAX, file, DIM)\n",
        "  y_train = to_categorical(y_train)\n",
        "\n",
        "  return MAX, X_temb, y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHFnIW-F35zw"
      },
      "source": [
        "### Model Construct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJzZLcIA534V"
      },
      "outputs": [],
      "source": [
        "def lstm(MAX, DIM):\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(units=64, input_shape=(MAX, DIM), return_sequences=True))\n",
        "  model.add(Dropout(0.3))\n",
        "  model.add(LSTM(units=32))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(LayerNormalization())\n",
        "  model.add(Dense(units=10, activation='relu'))\n",
        "  model.add(Dense(units=6, activation='softmax'))\n",
        "\n",
        "  model.compile(optimizer='adam', loss=keras.losses.categorical_crossentropy, metrics=['acc'])\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jdXdGjh838t-"
      },
      "source": [
        "### Different glove file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfUwYdEW7zQk",
        "outputId": "5b4f9179-bae7-46ba-a3bf-60b871775d65"
      },
      "outputs": [],
      "source": [
        "for d in DIM:\n",
        "  print('dim :',d)\n",
        "  MAX, X_temb, y_train = w2vector(data, d)\n",
        "  model = lstm(MAX, d)\n",
        "  model.fit(X_temb, y_train, validation_split=0.2, batch_size=128, epochs=5, verbose=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4PsiIrs4QX2"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtOE5MpGOBH3",
        "outputId": "8b8a9ac6-6b00-4a05-f651-406c8437e8cd"
      },
      "outputs": [],
      "source": [
        "MAX, X, y = w2vector(data, 300)\n",
        "kf = KFold(n_splits=3,shuffle=True)\n",
        "\n",
        "acc = 0\n",
        "rec = [0,0,0,0,0,0]\n",
        "prec = [0,0,0,0,0,0]\n",
        "f1 = [0,0,0,0,0,0]\n",
        "matrix = np.zeros((6,6)).tolist()\n",
        "for train, test in kf.split(X):\n",
        "  x_train = X[train]\n",
        "  y_train = y[train]\n",
        "  x_test = X[test]\n",
        "  y_test = [np.argmax(y[test][i]) for i in range(len(y[test]))]\n",
        "\n",
        "  model = lstm(MAX, 300)\n",
        "  model.fit(x_train, y_train, validation_split=0, batch_size=256, epochs=10, verbose=1)\n",
        "  result = model.predict(x_test)\n",
        "  y_pred = [np.argmax(result[i]) for i in range(len(result))]\n",
        "\n",
        "  acc += accuracy_score(y_test,y_pred)\n",
        "  rec += recall_score(y_test,y_pred,average=None)\n",
        "  prec += precision_score(y_test,y_pred,average=None)\n",
        "  f1 += f1_score(y_test,y_pred,average=None)\n",
        "  matrix += confusion_matrix(y_test,y_pred)\n",
        "acc /= 3\n",
        "rec /= 3\n",
        "prec /= 3\n",
        "f1 /= 3\n",
        "matrix /= 3\n",
        "\n",
        "matrix = matrix.tolist()\n",
        "for i in matrix:\n",
        "  for j in range(len(i)):\n",
        "    i[j] = round(i[j])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxHblFE8459I"
      },
      "source": [
        "### Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C_HFQzF71iwZ"
      },
      "outputs": [],
      "source": [
        "# [üòú,üòç,üòâ,üî•,üíú,üíØ]\n",
        "print('accrucy score :',acc)\n",
        "\n",
        "form = pd.DataFrame([rec,prec,f1],columns=[i for i in range(6)],index=['recall score','precision score','f1 score'])\n",
        "display(form)\n",
        "\n",
        "form = pd.DataFrame(matrix,[i for i in range(6)],[i for i in range(6)])\n",
        "sb.heatmap(form,annot=True,fmt='d',linewidth=5,cmap='YlGnBu')\n",
        "plt.xlabel('predicted')\n",
        "plt.ylabel('real')\n",
        "plt.title('confusion matrix')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "AI_project1(emoji).ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
